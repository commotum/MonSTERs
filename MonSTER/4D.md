- For ARC
    - remove translational offset
    - verify we can get it in a consistent corner
    - create rotated puzzle (windmill)
    - create mirrored puzzle (flipped)
    - Is the size of the grid baked into the model?
        - Theoretically, no. Practically, yes.
- For SUDOKU

export WANDB_PROJECT=sudoku-pos-enc
export WANDB_NAME=monster
export WANDB_GROUP=sudoku-pos-enc

OMP_NUM_THREADS=8 python pretrain.py \
  data_path=data/sudoku-extreme-15k-aug-1000 \
  epochs=20000 \
  eval_interval=2000 \
  global_batch_size=2048 \
  lr=7e-5 \
  puzzle_emb_lr=7e-5 \
  weight_decay=1.0 \
  puzzle_emb_weight_decay=1.0 \
  arch.pos_encodings=rope

OMP_NUM_THREADS=8 python pretrain.py \
  data_path=data/sudoku-extreme-1k-aug-1000 \
  epochs=20000 \
  eval_interval=2000 \
  global_batch_size=384 \
  lr=7e-5 \
  puzzle_emb_lr=7e-5 \
  weight_decay=1.0 \
  puzzle_emb_weight_decay=1.0 \
  arch.pos_encodings=monster

python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-10k-aug-1000  --subsample-size 10000 --num-aug 1000


Your long pause every ~244 steps still isn’t explained by eval_interval=500, because 244 ≠ 500. That pattern screams end-of-epoch overhead (DataLoader worker respawn, epoch-end hooks, maybe eval-on-epoch) rather than your step-based evals. So:

Lower eval_interval → additional pauses at 500, 1000, 1500…

The big slowdowns at 244, 488, 732… → likely epoch boundaries.

If you want to verify quickly:

Temporarily set eval very rare (e.g., eval_interval=999999) and see if the 244-step stalls still happen. If they do, it’s not eval; it’s epoch-end (DataLoader, saving, or logging).

Add persistent_workers=True to your DataLoader(s); that’s the usual fix for epoch stalls.