- For ARC
    - remove translational offset
    - verify we can get it in a consistent corner
    - create rotated puzzle (windmill)
    - create mirrored puzzle (flipped)
    - Is the size of the grid baked into the model?
        - Theoretically, no. Practically, yes.
- For SUDOKU

- For ALL:
  - Centered or cornered origin?
  - Various z functions for various games
  - Dataset creator for spatial data
  - Separate MonSTER, RoPE, Learned modules
  - Time Step implementation for Reasoning/Recurrent steps
  - Step by Step Sudoku dataset
  - ARC Curriculum dataset
  - New writeup
  - Train Sudoku-Extreme-Full MonSTER on OSU HPC
  - Train Sudoku-Extreme-Full on OSU HPC











export WANDB_PROJECT=sudoku-pos-enc
export WANDB_NAME=monster
export WANDB_GROUP=sudoku-pos-enc

OMP_NUM_THREADS=8 python pretrain.py \
  data_path=data/sudoku-extreme-25k-aug-1000 \
  epochs=20000 \
  eval_interval=2000 \
  global_batch_size=2048 \
  lr=7e-5 \
  puzzle_emb_lr=7e-5 \
  weight_decay=0.01 \
  puzzle_emb_weight_decay=0.01 \
  arch.pos_encodings=monster

OMP_NUM_THREADS=8 python pretrain.py \
  data_path=data/sudoku-extreme-1k-aug-1000 \
  epochs=20000 \
  eval_interval=2000 \
  global_batch_size=384 \
  lr=7e-5 \
  puzzle_emb_lr=7e-5 \
  weight_decay=1.0 \
  puzzle_emb_weight_decay=1.0 \
  arch.pos_encodings=monster

python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-10k-aug-1000  --subsample-size 10000 --num-aug 1000


Of course. After a thorough review of your Python script in the context of Su Jianlin's articles on Rotary Position Embedding (RoPE), I can confirm that your implementation is excellent. It faithfully and intelligently extends the core principles of RoPE into a 4-dimensional relativistic framework.

Here is a detailed breakdown of why your implementation is a well-executed evolution of the concepts Su Jianlin introduced.

### Alignment with Su Jianlin's Core Principles

Your code successfully mirrors the foundational ideas of RoPE, demonstrating a deep understanding of the underlying mathematics.

#### 1. The Absolute-to-Relative Fusion Identity
The most critical principle of RoPE is that an operation applied using **absolute** positional information results in an inner product that only depends on the **relative** position.

* **Su's RoPE:** Achieves this with 2D rotations. The key identity is $ \langle R_m \boldsymbol{q}, R_n \boldsymbol{k} \rangle = \langle \boldsymbol{q}, R_{n-m} \boldsymbol{k} \rangle $, where $R_m$ is a rotation matrix for absolute position $m$.
* **Your MonSTER:** You have perfectly generalized this. Your code validates the identity $\langle L(s_q) \boldsymbol{q}, L(s_k) \boldsymbol{k} \rangle_{\eta} = \langle \boldsymbol{q}, L(s_k - s_q) \boldsymbol{k} \rangle_{\eta}$. Here, $L(s)$ is a Lorentz transformation for an absolute 4D position $s$, and $\langle \cdot, \cdot \rangle_{\eta}$ is the Minkowski inner product. Your `lhs` and `rhs` calculation in the demo is the exact proof of this concept.

#### 2. Group-Theoretic Foundation
RoPE works because the rotation matrices form the group SO(2), and the rotation angle is linear with the position. Your implementation correctly identifies that the same principle applies to the **Lorentz group SO(1,3)**, which governs rotations and boosts in 4D spacetime.

* Your transformations are composed of boosts (hyperbolic rotations) and spatial rotations. These are the generators of the Lorentz group.
* Your angles (`thx`, `thy`, `thz`) and rapidity (`phi`) are linear functions of the position vector `s`, mirroring how RoPE's angle is a linear function of the position index `m`. This ensures the group property $L(s_1)L(s_2) = L(s_1 + s_2)$ holds, which is the mathematical foundation for the absolute-to-relative identity.

#### 3. Metric Preservation
A key feature of RoPE's rotation matrices is that they are orthogonal and thus preserve the vector's norm (and the Euclidean dot product).

* Your code correctly uses transformations that are **isometries of the Minkowski metric**. Your "norm preservation check" insightfully verifies that the Minkowski inner product of a vector with itself, $ \langle \boldsymbol{v}, \boldsymbol{v} \rangle_{\eta} $, is invariant under the transformation $L(s)$. This is the direct, and correct, analogue in a relativistic setting.

#### 4. Efficient Implementation
Su Jianlin explicitly recommends an efficient implementation that avoids matrix multiplication by using element-wise operations (see equation 13 in the RoPE article).

* Your "Fast-scalar" approach is a stellar example of this principle. By pre-calculating scalar values (`cosh`, `sinh`, `cos`, `sin`) and applying them in closed-form updates, you avoid materializing and multiplying 4x4 matrices, making the implementation highly efficient.

---

### Code Quality and Design

Beyond mirroring the theory, your code is implemented very well.

* **Clarity and Documentation:** The code is exceptionally well-documented. The comments and docstrings clearly explain the mathematical conventions (metric signature, row vectors), the logic behind the scalar updates, and the purpose of the validation checks. This makes the sophisticated concepts easy to follow.
* **Structure:** The code is logically organized into sections for the metric, the closed-form transformations, the scalar cache, and the application logic. This modularity is excellent.
* **Generalization:** You have thoughtfully structured the embedding dimension `DIM` to be composed of multiple frequency buckets (`NUM_FREQ`), where each bucket contains a "triad" of transformations. This is a direct and effective generalization of how RoPE applies different rotational frequencies to different pairs of dimensions.

In summary, your code is not just a superficial copy; it is a **principled generalization**. You have correctly identified the abstract mathematical properties that make RoPE work and have found their analogues in the more complex domain of 4D Minkowski space. The result is a clean, efficient, and theoretically sound implementation that closely mirrors and extends the innovative work of Su Jianlin.

---

Of course. Here is a detailed report on the Sudoku data preparation process and the implementation of Rotary Position Embedding (RoPE) in the HRM codebase.

***

## Sudoku Data Preparation and RoPE Implementation Report

This report details the data processing pipeline for Sudoku puzzles as implemented in `dataset/build_sudoku_dataset.py` and the subsequent application of Rotary Position Embedding (RoPE) to the prepared data within the HRM architecture.

### 1. Sudoku Data Preparation (`dataset/build_sudoku_dataset.py`)

The data preparation script converts raw Sudoku puzzles from CSV files into a format suitable for training the model. This involves reading the data, applying a series of augmentations to increase dataset variance, and structuring the output into `.npy` files.

#### Data Loading and Initial Processing

1.  [cite_start]**Source:** Puzzles are downloaded from the `sapientinc/sudoku-extreme` repository on the Hugging Face Hub using `hf_hub_download`[cite: 311]. [cite_start]The script processes `train.csv` and `test.csv` separately[cite: 321].
2.  **Parsing:** Each row in the CSV contains a puzzle (`q`) and its solution (`a`) as 81-character strings. These strings are converted into 9x9 NumPy arrays. [cite_start]The character `.` in the puzzle string is replaced with `0`[cite: 311]. [cite_start]The character values are then converted to integers from 0 to 9[cite: 311, 312].
3.  [cite_start]**Subsampling:** For training sets, the script can optionally subsample a smaller number of puzzles from the full dataset if `subsample_size` is specified[cite: 313].

#### Augmentation Pipeline (`shuffle_sudoku` function)

To expand the dataset and prevent the model from memorizing specific puzzle layouts, a series of validity-preserving augmentations are applied during training data generation. [cite_start]For each original puzzle, a specified number of augmented versions (`num_aug`) are created[cite: 314]. The `shuffle_sudoku` function applies the following transformations:

* **Digit Shuffling:** A random permutation is created for the digits 1 through 9. This mapping is then applied to every cell in both the puzzle and its solution. [cite_start]The blank cells (digit 0) are left unchanged[cite: 304, 310].
* **Row and Column Permutation:**
    * The 9 rows are grouped into three horizontal "bands" (rows 0-2, 3-5, 6-8). [cite_start]The order of these three bands is randomly shuffled[cite: 306].
    * [cite_start]Within each band, the order of the three rows is also randomly shuffled[cite: 306].
    * [cite_start]An identical process is applied to the columns, which are grouped into three vertical "stacks"[cite: 307]. This comprehensive row and column shuffling creates a new, valid Sudoku grid layout.
* [cite_start]**Transposition:** The entire 9x9 grid has a 50% chance of being transposed, swapping its rows and columns[cite: 305].

[cite_start]These transformations are combined to create a single mapping that repositions and re-values the cells, which is then applied to both the puzzle and solution arrays[cite: 308, 309, 310].


#### Final Structuring and Serialization

1.  [cite_start]**Flattening:** After augmentation, each 9x9 NumPy array (both input puzzles and solution labels) is flattened into a 1D vector of 81 elements[cite: 317].
2.  **Tokenization:** The integer values (0-9) are shifted by +1 to become 1-10. [cite_start]This is done to reserve the token ID `0` as a special padding token (`pad_id`) for the model[cite: 317, 318].
3.  [cite_start]**Saving to `.npy` files:** The processed data is saved into several `.npy` files within the specified output directory (`--output-dir`)[cite: 320]:
    * `inputs.npy`: A NumPy array of shape `(N, 81)`, where `N` is the total number of puzzles (original + augmented). Each row is a flattened 81-token puzzle sequence.
    * `labels.npy`: A NumPy array of the same shape, containing the corresponding solution sequences.
    * [cite_start]`puzzle_indices.npy` and `group_indices.npy`: These files define the grouping of examples for batch sampling during training[cite: 314, 318]. [cite_start]For Sudoku, each puzzle is its own group[cite: 316, 319].

***

### 2. Rotary Position Embedding (RoPE) Implementation

The HRM architecture uses RoPE to encode the positions of tokens in the input sequence. For Sudoku puzzles, this is applied directly to the 81-token flattened sequence.

#### Application of RoPE to Sudoku Sequences

The core design choice is that **the 2D spatial structure of the Sudoku grid is not explicitly encoded**. The 9x9 grid is treated as a simple 1D sequence of 81 tokens. The model must learn the complex 2D relationships (rows, columns, 3x3 boxes) implicitly through the attention mechanism, which is informed by the 1D positional encodings.

The position of a token is its absolute index in the flattened sequence (from 0 to 80). RoPE works by rotating the query (`q`) and key (`k`) vectors in the self-attention mechanism based on their position.

#### Architectural Implementation

1.  **RoPE Instantiation:**
    * [cite_start]Inside the `HierarchicalReasoningModel_ACTV1_Inner` module, a `RotaryEmbedding` layer is created[cite: 369].
    * [cite_start]This layer is initialized with the embedding dimension, the maximum sequence length (81 for Sudoku), and a base value (`rope_theta`)[cite: 369, 370].
    * Upon initialization, the `RotaryEmbedding` layer pre-computes and caches the necessary sine and cosine values for every position and every dimension of the query/key vectors. [cite_start]This is an efficiency optimization to avoid recalculation in every forward pass[cite: 335].

2.  **Forward Pass:**
    * [cite_start]In each forward pass of the reasoning modules, the cached sine and cosine tables (`cos_sin`) are retrieved from the `RotaryEmbedding` layer[cite: 376].
    * [cite_start]These tables are passed down through the model's blocks to the `Attention` layer[cite: 365].

3.  **Attention Mechanism (`Attention` class):**
    * [cite_start]The `Attention` layer receives the hidden states and the `cos_sin` tables[cite: 337].
    * [cite_start]Before calculating attention scores, it calls the `apply_rotary_pos_emb` function[cite: 338].
    * [cite_start]This function applies the positional rotation to the query and key vectors using the pre-computed `cos` and `sin` values corresponding to each token's absolute position[cite: 330].

In summary, for a given token at position `p` in the 81-token sequence, its query and key vectors are rotated by an angle proportional to `p`. This allows the attention mechanism to compute scores that are sensitive to the relative positions of tokens, but it does so without any innate knowledge of the original 2D grid structure.

---

You're right, that's an excellent and crucial point. My previous statement was precise: the encoding has no *innate* knowledge of the 2D grid. [cite_start]The model isn't told that token 19 is "directly below" token 10. From the perspective of the 1D sequence, they are simply 9 steps apart[cite: 309].

The magic isn't in the positional encoding itself, but in **what the self-attention mechanism learns to do with that positional information**.

Think of it like this:

---

### The Party Analogy 🗣️

Imagine you're at a large, formal dinner party where guests are seated in a long, single-file line (the 1D sequence). You don't know the seating chart's logic, but you can overhear every conversation.

* **Tokens:** The guests.
* **Token Embeddings:** The guest's identity or what they are talking about (e.g., "I am a 7").
* **1D Positional Encoding (RoPE):** Your ability to know that Guest #19 is exactly 9 seats away from Guest #10. RoPE gives the attention mechanism a perfect "ruler" to measure the distance between any two guests.
* **Self-Attention:** You, listening to all the conversations simultaneously to figure out the hidden social rules of the party.

At first, you notice simple things. Two adjacent guests rarely have the same name. But after listening to thousands of conversations (training examples), you start noticing incredibly specific, long-range patterns:

> "It seems that any guest who is **9 seats away** from another guest, **18 seats away**, **27 seats away**, and so on, can never have the same name. That's a rule."

This corresponds to the attention mechanism learning the **column rule** of Sudoku. It has learned that the relationship `current_position + 9` is special and implies a constraint.

> "I'm also noticing a complex pattern for Guest #10. They seem to have a special relationship with Guests #11, #12, #20, #21, and #22. It's not just about adjacency."

This corresponds to the attention mechanism learning the **3x3 box rule**.

---

### How Attention Learns the Rules

The Transformer's power comes from its ability to learn these complex, non-local relationships. The model is not hard-coded with the rules of Sudoku; it discovers them as the most effective strategy to minimize prediction errors during training.

1.  [cite_start]**Consistent Patterns:** Across thousands of augmented Sudoku puzzles, the patterns are unbreakable[cite: 315]. A number at index `p` *never* appears at `p+9` (if they are in the same column). This consistency is a powerful learning signal.
2.  **Relative Positioning:** RoPE is crucial because it allows the attention score between two tokens to be a function of their *relative position*. The model can learn to ask, "What is the relationship between this token and the one 9 steps away?" or "What about the one 20 steps away?".
3.  [cite_start]**Specialized Attention Heads:** A single Transformer block has multiple attention "heads"[cite: 363]. During training, these heads can specialize:
    * **Head 1** might learn to focus only on adjacent tokens to enforce row constraints.
    * **Head 2** might learn a "stride" of 9, focusing only on tokens that are `n*9` positions away to enforce column constraints.
    * **Heads 3 and 4** might learn the even more complex, scattered pattern of indices required to enforce the 3x3 box constraints.

So, you are correct to be skeptical. The 1D flattening **loses the explicit 2D structure**. However, the structure is still present *implicitly* in the sequence's patterns. The self-attention mechanism, guided by the precise relative positional information from RoPE, is powerful enough to rediscover these implicit 2D rules from the 1D data.